# -*- coding: utf-8 -*-
"""Final Code for Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10wsDkTXBXKLevHW_H6boOK8Q9KFah3tv
"""

import os
import pickle
import numpy as np
from music21 import note, chord, corpus, converter, stream, instrument
import glob
from tensorflow.keras.utils import to_categorical
from tqdm.notebook import tqdm

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import LSTM, Input, Dropout, Dense, Activation, Embedding, Concatenate, Reshape
from tensorflow.keras.layers import Flatten, RepeatVector, Permute, TimeDistributed
from tensorflow.keras.layers import Multiply, Lambda, Softmax
import tensorflow.keras.backend as K 
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import RMSprop

!unzip midi_dataset.zip

data_dir = 'midi_dataset' 


# list of files
midi_list = os.listdir(data_dir) # get the data directory 

# Load and make list of stream objects
original_scores = []
for midi in tqdm(midi_list):
    score = converter.parse(os.path.join(data_dir,midi)) # pass the full part of the file
    original_scores.append(score)

midi_list

original_scores

original_scores = [midi.chordify() for midi in tqdm(original_scores)]

original_scores

from traitlets.traitlets import List
# Define empty lists of lists
original_chords = [[] for _ in original_scores]
original_durations = [[] for _ in original_scores]
original_velocities = [[] for _ in original_scores]
original_keys = []

# Extract notes, chords, durations, and keys
for i, midi in tqdm(list(enumerate(original_scores))):# unique identifier getting index
    original_keys.append(str(midi.analyze('key')))
    for element in midi:
        if isinstance(element, note.Note): # isinstance is true or false
            original_velocities[i].append(element.volume.velocity)
            original_chords[i].append(element.pitch) # element = note is true 
            original_durations[i].append(element.duration.quarterLength)
        elif isinstance(element, chord.Chord):
            original_velocities[i].append(element.volume.velocity)
            original_chords[i].append('.'.join(str(n) for n in element.pitches))# . will concatenate into one chord
            original_durations[i].append(element.duration.quarterLength)

original_velocities

(original_chords)

(original_durations)

original_keys

major_chords = [c for (c, k) in tqdm(list(zip(original_chords, original_keys))) if (k == 'G major')]
major_durations = [d for (d, k) in tqdm(list(zip(original_durations, original_keys))) if (k == 'G major')]
major_velocities = [v for (v, k) in tqdm(list(zip(original_velocities, original_keys))) if (k == 'G major')]

len(major_chords)

len(major_durations)

len(major_velocities)

def get_distinct(elements):
    # Get all pitch names
    element_names = sorted(set(elements))
    n_elements = len(element_names)
    return (element_names, n_elements) # outcome is a set of unique element and the number of unique elements

def create_lookups(element_names):
    # create dictionary to map notes and durations to integers
    element_to_int = dict((element, number) for number, element in enumerate(element_names))
    int_to_element = dict((number, element) for number, element in enumerate(element_names))

    return (element_to_int, int_to_element)

store_folder = 'parsed_data'
os.mkdir(store_folder)

store_folder

# get the distinct sets of notes and durations
note_names, n_notes = get_distinct([n for chord in major_chords for n in chord])
duration_names, n_durations = get_distinct([d for dur in major_durations for d in dur])
velocity_names, n_velocities = get_distinct([v for vel in major_velocities for v in vel])
distincts = [note_names, n_notes, duration_names, n_durations,velocity_names, n_velocities]

with open(os.path.join(store_folder, 'distincts'), 'wb') as f:
    pickle.dump(distincts, f)

len([n for chord in major_chords for n in chord])


#l = []

#for song in major_songs:
  #for chord in song:
    #l.append(chord)

(n_notes)

distincts

duration_names[-1]

n_durations

velocity_names[1]

# make the lookup dictionaries for notes and dictionaries and save
note_to_int, int_to_note = create_lookups(note_names)
duration_to_int, int_to_duration = create_lookups(duration_names)
velocity_to_int, int_to_velocity = create_lookups(velocity_names)
lookups = [note_to_int, int_to_note, duration_to_int, int_to_duration, velocity_to_int, int_to_velocity]

with open(os.path.join(store_folder, 'lookups'), 'wb') as f:
    pickle.dump(lookups, f)

lookups

print("Unique Notes={}, Duration values={} and Velocity values={}".format(n_notes,n_durations,n_velocities))

# Set sequence length
sequence_length = 32  

# Define empty array for train data
train_chords = []
train_durations = []
train_velocities = []
target_chords = []
target_durations = []
target_velocities = []

# Construct train and target sequences for chords and durations
for s in range(len(major_chords)):
    chord_list = [note_to_int[c] for c in major_chords[s]]
    duration_list = [duration_to_int[d] for d in major_durations[s]]
    velocity_list = [velocity_to_int[d] for d in major_velocities[s]]
    for i in range(len(chord_list) - sequence_length):# eg: 100-10 : 90 sequences
        train_chords.append(chord_list[i:i+sequence_length]) # takes the input of sequence lengtth and moves right in every slide
        train_durations.append(duration_list[i:i+sequence_length])
        train_velocities.append(velocity_list[i:i+sequence_length])
        target_chords.append(chord_list[i+1])
        target_durations.append(duration_list[i+1])
        target_velocities.append(velocity_list[i+1])

target_durations[0]

train_durations

target_chords[2]

train_chords[2]

target_velocities

train_chords = np.array(train_chords)
train_durations = np.array(train_durations)
train_velocities = np.array(train_velocities)
target_chords = np.array(target_chords)
target_durations = np.array(target_durations)
target_velocities = np.array(target_velocities)

target_velocities

train_velocities

target_durations

target_chords

train_chords

train_durations

def create_network(n_notes, n_durations, n_velocities, embed_size = 100, rnn_units = 256):
    """ create the structure of the neural network """

    notes_in = Input(shape = (None,)) #shape of the data we need to pass, its the first dimention, how many times steps in the sequence, in our case its 32
    durations_in = Input(shape = (None,))# we are passing to the model, none enable us to have as many time steps we want in the network
    velocities_in = Input(shape = (None,)) # we can have melody of any length 

    x1 = Embedding(n_notes, embed_size)(notes_in) #n_notes = is the size of vocabullary, embed_size= shape of embedded vector
    x2 = Embedding(n_durations, embed_size)(durations_in) #This layer will convert any of the feature input into a vector of the shape of the output dimension.
    x3 = Embedding(n_velocities, embed_size)(velocities_in)
    
    x = Concatenate()([x1,x2,x3])

    x = LSTM(rnn_units, return_sequences=True)(x)
    x = Dropout(0.2)(x)
    x = LSTM(rnn_units, return_sequences=True)(x)
    x = Dropout(0.2)(x)
  
    
# attention

    e = Dense(1, activation='tanh')(x) #
    e = Reshape([-1])(e)
    alpha = Activation('softmax')(e)

    alpha_repeated = Permute([2, 1])(RepeatVector(rnn_units)(alpha))

    c = Multiply()([x, alpha_repeated])
    c = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(rnn_units,))(c)
    
    notes_out = Dense(n_notes, activation = 'softmax', name = 'pitch')(c)
    durations_out = Dense(n_durations, activation = 'softmax', name = 'duration')(c)
    velocities_out = Dense(n_velocities, activation = 'softmax', name = 'velocity')(c)
    model = Model([notes_in, durations_in, velocities_in], [notes_out, durations_out, velocities_out])
    model.compile(loss=['sparse_categorical_crossentropy', 'sparse_categorical_crossentropy',
                        'sparse_categorical_crossentropy'], optimizer=RMSprop(learning_rate = 0.001))

    return model

embed_size = 128
rnn_units = 128

model = create_network(n_notes, n_durations, n_velocities, embed_size, rnn_units)

model.summary()

os.mkdir('outputs')
  os.mkdir('outputs/output')
  os.mkdir('outputs/weights')

output_folder = 'outputs'
weights_folder = os.path.join(output_folder, 'weights')


checkpoint = ModelCheckpoint(
    os.path.join(weights_folder, "weights.h5"),
    monitor='loss',
    verbose=0,
    save_best_only=True,
    mode='min'
)

epoch_checkpoint = ModelCheckpoint(
    os.path.join(weights_folder, "weights-{epoch:02d}-{loss:.4f}.h5"),
    monitor='loss',
    verbose=0,
    save_best_only=True,
    mode='min'
)

early_stopping = EarlyStopping(
    monitor='loss'
    , restore_best_weights=True
    , patience = 5
)


callbacks_list = [
    checkpoint
    , epoch_checkpoint
    , early_stopping
 ]

model.save_weights(os.path.join(weights_folder, "weights.h5"))

history = model.fit([train_chords, train_durations, train_velocities], 
                    [target_chords, target_durations, target_velocities]
                    , epochs=50, batch_size=128 # is the amount of sample network will see before running back propogation(mini batch gradient)
                    , callbacks=callbacks_list
                    , shuffle=True
                  )

SAVE_MODEL_PATH = "model.h5" 
model.save(SAVE_MODEL_PATH)

from matplotlib import pyplot as plt

8# get plots
plt.plot(history.history['pitch_loss'])
plt.plot(history.history['duration_loss'])
plt.plot(history.history['velocity_loss'])
plt.show()

print(history.history.keys())

def sample_with_temp(preds, temperature):# range , the way we sample output symbols from the prob. distribution that comes out of network

    if temperature == 0:
        return np.argmax(preds)
    else:
        preds = np.log(preds) / temperature
        exp_preds = np.exp(preds)
        preds = exp_preds / np.sum(exp_preds)
        return np.random.choice(len(preds), p=preds)

# chord, velocity and duration sequences
initial_chords = np.expand_dims(train_chords[0,:].copy(), 0) # first song  , : = all elements inside it
initial_durations = np.expand_dims(train_durations[0,:].copy(), 0)
initial_velocities = np.expand_dims(train_velocities[0,:].copy(), 0)

initial_velocities

'''import math
rounded_velocity = np.array([10*math.floor(v / 10) for v in initial_velocities[0]])
initial_velocities = np.expand_dims(rounded_velocity.copy(), 0)
initial_velocities
#
#train_velocities[-1,:].copy()'''

initial_velocities

initial_chords

initial_durations

# Function to predict velocity, chords and durations
def predict_chords(chord_sequence, duration_sequence, velocity_sequence, model,temperature=1.0):
    predicted_chords, predicted_durations, predicted_velocities  = model.predict([chord_sequence, duration_sequence, velocity_sequence])
    return sample_with_temp(predicted_chords[0],temperature),sample_with_temp(predicted_durations[0],temperature), sample_with_temp(predicted_velocities[0],temperature)

# Define empty lists for generated chords and durations
new_chords, new_durations, new_velocities = [], [], []

# Generate chords and durations using 100 rounds of prediction
for j in range(100):
    new_chord, new_duration, new_velocity = predict_chords(initial_chords, 
                                             initial_durations,
                                             initial_velocities,
                                             model,
                                             temperature=2.0)
    new_chords.append(new_chord)
    new_durations.append(new_duration)
    new_velocities.append(new_velocity)
    initial_chords[0][:-1] = initial_chords[0][1:] # fill up the beginning of the array except the last , setting the last value in the array to the new predicted chord
    initial_chords[0][-1] = new_chord # 
    initial_durations[0][:-1] = initial_durations[0][1:]
    initial_durations[0][-1] = new_duration
    initial_velocities[0][:-1] = initial_velocities[0][1:]
    initial_velocities[0][-1] = new_velocity

'''new_velocities
import math

def roundup(x):
    return int(math.ceil(x / 10.0)) * 10
roundup(new_velocities)'''

'''# Define empty lists for generated chords and durations
new_chords, new_durations = [], []

# Generate chords and durations using 50 rounds of prediction
for j in range(50):
    new_chord, new_duration = predict_chords(initial_chords, 
                                             initial_durations,
                                             model,
                                             temperature=0.8)
    new_chords.append(new_chord)
    new_durations.append(new_duration)
    initial_chords[0][:-1] = initial_chords[0][1:]
    initial_chords[0][-1] = new_chord
    initial_durations[0][:-1] = initial_durations[0][1:]
    initial_durations[0][-1] = new_duration'''

(new_chords)

(new_durations)

new_velocities

'''song_chords = initial_chords.copy()
song_durs = initial_durations.copy()

for j in range(200):
  new_chord, new_duration = predict_chords(song_chords[-10:], 
                                             song_durs[-10:],
                                             model,
                                             temperature=0.8)
  song_chords.append(new_chord)
  song_durs.append(new_duration)'''

# Create stream object and add piano as instrument
generated_stream = stream.Stream()
generated_stream.append(instrument.Piano())

# Add notes and durations to stream
for j in range(len(new_chords)):

    try:
        n = note.Note(int_to_note[new_chords[j]].replace('.', ' '), 
                                          quarterType = int_to_duration[new_durations[j]])
        n.volume.velocity = int_to_velocity[new_velocities[j]]
        generated_stream.append(n)
    except:
        c = chord.Chord(int_to_note[new_chords[j]].replace('.', ' '), 
                                            quarterType = int_to_duration[new_durations[j]])
        c.volume.velocity = int_to_velocity[new_velocities[j]]
        generated_stream.append(c)

# Export as MIDI file
generated_stream.write('midi', fp='lstm_50epoch.mid')

#print(generated_stream)

int_to_velocity[new_velocities[10]]

!apt install fluidsynth
!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2
!pip install midi2audio
from midi2audio import FluidSynth
from IPython.display import Audio

FluidSynth("font.sf2").midi_to_audio('lstm_50epoch.mid', 
                                     'lstm_50epoch.wav')

Audio("lstm_50epoch.wav")

!zip -r outputs.zip outputs

!ls -ltrh

!pip install mido==1.2.9

from mido import MidiFile

mid = MidiFile('lstm_50epoch.mid', clip=True)
print(mid)

'''FluidSynth("font.sf2").midi_to_audio('for_elise_by_beethoven.mid', 
                                     'for_elise_by_beethoven.wav')'''

for track in mid.tracks:
    print(track)

for msg in mid.tracks[0]:
    print(msg)

import mido
mid = mido.MidiFile('lstm_50epoch.mid', clip=True)
mid.tracks

mid.length

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

# Load in sounds
music1, sr = librosa.load('/content/50 Epochs F minor with velo 0.8 temp.wav')
music2, _= librosa.load('/content/50 Epochs C major with velo 0.8 temp.wav')
music3, _ = librosa.load('/content/50 Epochs G major with velo 0.8 temp.wav')

#Visualizing waveforms
fig, ax = plt.subplots(1,3, figsize = (20,5), sharey = True)
librosa.display.waveplot(music1, sr=sr, ax=ax[0])
ax[0].set(title = 'F minor')
librosa.display.waveplot(music2, sr=sr, ax=ax[1])
ax[1].set(title = 'C major')
librosa.display.waveplot(music3, sr=sr, ax=ax[2])
ax[2].set(title = 'G Major')
plt.show()

music1_chroma = librosa.feature.chroma_stft(music1, sr=sr)
music2_chroma = librosa.feature.chroma_stft(music2, sr=sr)
music3_chroma = librosa.feature.chroma_stft(music3, sr=sr)

# Visualize the STFT chromagrams
fig, ax = plt.subplots(1,3, figsize=(30,15))
img = librosa.display.specshow(music1_chroma, y_axis = 'chroma', x_axis='time', ax=ax[0])
ax[0].set(title = 'Fminor 0.8')
librosa.display.specshow(music2_chroma, y_axis = 'chroma', x_axis='time', ax=ax[1])
ax[1].set(title = 'C major 0.8')
librosa.display.specshow(music3_chroma, y_axis = 'chroma', x_axis='time', ax=ax[2])
ax[2].set(title = 'G major 0.8')
fig.colorbar(img, ax=ax)
plt.show()